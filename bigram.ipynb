{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6971302,"sourceType":"datasetVersion","datasetId":4005378}],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-15T06:34:31.725038Z","iopub.execute_input":"2023-11-15T06:34:31.725308Z","iopub.status.idle":"2023-11-15T06:34:32.067437Z","shell.execute_reply.started":"2023-11-15T06:34:31.725282Z","shell.execute_reply":"2023-11-15T06:34:32.066552Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/text-for-llm/wizard_of_oz.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nblock_size = 8\nbatch_size = 4\nmax_iters = 100000\nlearning_rate = 3e-4\neval_iters =1000","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:26.065378Z","iopub.execute_input":"2023-11-15T07:11:26.065803Z","iopub.status.idle":"2023-11-15T07:11:26.072359Z","shell.execute_reply.started":"2023-11-15T07:11:26.065773Z","shell.execute_reply":"2023-11-15T07:11:26.071478Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/input/text-for-llm/wizard_of_oz.txt','r',encoding='utf-8') as f:\n    text=f.read()\nchars=sorted(set(text))\nprint(chars)\nvocab_size = len(chars)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:26.241195Z","iopub.execute_input":"2023-11-15T07:11:26.241500Z","iopub.status.idle":"2023-11-15T07:11:26.253185Z","shell.execute_reply.started":"2023-11-15T07:11:26.241473Z","shell.execute_reply":"2023-11-15T07:11:26.252277Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n","output_type":"stream"}]},{"cell_type":"code","source":"#character to integer encode and decode\n#character level tokenizer \nstring_to_int={ch:i for i,ch in enumerate(chars)}\nint_to_string={i:ch for i,ch in enumerate(chars)}\nencode=lambda s: [string_to_int[c] for c in s]\ndecode=lambda l: ''.join([int_to_string[i] for i in l])\ndata=torch.tensor(encode(text), dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:26.384448Z","iopub.execute_input":"2023-11-15T07:11:26.384741Z","iopub.status.idle":"2023-11-15T07:11:26.441376Z","shell.execute_reply.started":"2023-11-15T07:11:26.384717Z","shell.execute_reply":"2023-11-15T07:11:26.440677Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"n=int(0.8*len(data))\ntrain_data=data[:n]\nval_data=data[n:]\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x,y = x.to(device),y.to(device)\n    return x,y\n    \nx,y = get_batch('train')\nprint('inputs:')\nprint(x)\nprint('targets:')\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:26.542012Z","iopub.execute_input":"2023-11-15T07:11:26.542315Z","iopub.status.idle":"2023-11-15T07:11:26.553712Z","shell.execute_reply.started":"2023-11-15T07:11:26.542272Z","shell.execute_reply":"2023-11-15T07:11:26.552792Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"inputs:\ntensor([[76, 58, 71, 58,  1, 67, 68, 73],\n        [74, 67, 60,  1, 60, 62, 71, 65],\n        [58,  1, 47, 62, 79, 54, 71, 57],\n        [66, 54, 67,  9,  1, 72, 58, 71]], device='cuda:0')\ntargets:\ntensor([[58, 71, 58,  1, 67, 68, 73,  1],\n        [67, 60,  1, 60, 62, 71, 65,  1],\n        [ 1, 47, 62, 79, 54, 71, 57, 23],\n        [54, 67,  9,  1, 72, 58, 71, 62]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:26.691229Z","iopub.execute_input":"2023-11-15T07:11:26.691901Z","iopub.status.idle":"2023-11-15T07:11:26.697372Z","shell.execute_reply.started":"2023-11-15T07:11:26.691874Z","shell.execute_reply":"2023-11-15T07:11:26.696560Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, index, targets=None):\n        logits = self.token_embedding_table(index)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, index, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self.forward(index)\n            logits = logits[:, -1, :] \n            probs = F.softmax(logits, dim=-1) \n            index_next = torch.multinomial(probs, num_samples=1) \n            index = torch.cat((index, index_next), dim=1)\n        return index\n\nmodel = BigramLanguageModel(vocab_size)\nm = model.to(device)\n\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\ngenerated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\nprint(generated_chars)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:26.850060Z","iopub.execute_input":"2023-11-15T07:11:26.850667Z","iopub.status.idle":"2023-11-15T07:11:26.994388Z","shell.execute_reply.started":"2023-11-15T07:11:26.850633Z","shell.execute_reply":"2023-11-15T07:11:26.993396Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"\nErr7 vBaFWK[tzSKvqH'7sp6bcmMT;Vg7qd]x;V&NcF';V4i7﻿B!80B'0R0Huv2JWjIO[sI',;EX85g)04sH!'XUy&DT\"Wq-EG,oO6aTGYp.2ir99_'W*0'ah!USN\":*VdgWJZwk9O)xGDM*MgIBq4﻿_7ob,xbgaHV_I3Zy\nGfrgN_[q9_QgdY﻿[3C'8hgGURor4uJQC&1n;DhVK.uCjR._buWIw&xO8.s;V(zV9C*s)(JZwJ3Hy44J?c:QC 4:P\nXcmp_nKXUcb.EK2vf8NpQ3C8W o.2R﻿waFM-QZWhRzw;VK9p,EFcS_lIA4:1I)LFjRmpx6RDTmaF!b3IBH5d5l_Gn,IMGeP\nw\"W_'qM9:l6I:E85wSfi5OI33:&XDRVjg8hmPbgGkXUB.WhUaz\no(Zz_R5KPDg81lWGjWzwhvpmD:2kX'5R\"yIM!,8hi er6:GjgG﻿X(v1xdQmSyiSpQsz﻿l'fzwi2urBHW_HRUAnPUkolI3zwE\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_iters == 0:\n        losses = estimate_loss()\n        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n        \n    xb, yb = get_batch('train')\n\n    logits, loss = model.forward(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:11:27.328783Z","iopub.execute_input":"2023-11-15T07:11:27.329362Z","iopub.status.idle":"2023-11-15T07:14:03.331947Z","shell.execute_reply.started":"2023-11-15T07:11:27.329331Z","shell.execute_reply":"2023-11-15T07:14:03.331038Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"step: 0, train loss: 5.010, val loss: 5.018\nstep: 1000, train loss: 4.767, val loss: 4.764\nstep: 2000, train loss: 4.540, val loss: 4.541\nstep: 3000, train loss: 4.321, val loss: 4.317\nstep: 4000, train loss: 4.128, val loss: 4.133\nstep: 5000, train loss: 3.937, val loss: 3.943\nstep: 6000, train loss: 3.781, val loss: 3.795\nstep: 7000, train loss: 3.643, val loss: 3.638\nstep: 8000, train loss: 3.495, val loss: 3.501\nstep: 9000, train loss: 3.376, val loss: 3.392\nstep: 10000, train loss: 3.255, val loss: 3.272\nstep: 11000, train loss: 3.174, val loss: 3.195\nstep: 12000, train loss: 3.079, val loss: 3.104\nstep: 13000, train loss: 2.997, val loss: 3.022\nstep: 14000, train loss: 2.928, val loss: 2.948\nstep: 15000, train loss: 2.882, val loss: 2.889\nstep: 16000, train loss: 2.825, val loss: 2.846\nstep: 17000, train loss: 2.782, val loss: 2.794\nstep: 18000, train loss: 2.735, val loss: 2.773\nstep: 19000, train loss: 2.719, val loss: 2.732\nstep: 20000, train loss: 2.678, val loss: 2.708\nstep: 21000, train loss: 2.645, val loss: 2.674\nstep: 22000, train loss: 2.651, val loss: 2.674\nstep: 23000, train loss: 2.613, val loss: 2.643\nstep: 24000, train loss: 2.604, val loss: 2.628\nstep: 25000, train loss: 2.587, val loss: 2.609\nstep: 26000, train loss: 2.566, val loss: 2.582\nstep: 27000, train loss: 2.546, val loss: 2.570\nstep: 28000, train loss: 2.538, val loss: 2.582\nstep: 29000, train loss: 2.531, val loss: 2.577\nstep: 30000, train loss: 2.527, val loss: 2.565\nstep: 31000, train loss: 2.525, val loss: 2.551\nstep: 32000, train loss: 2.501, val loss: 2.550\nstep: 33000, train loss: 2.503, val loss: 2.537\nstep: 34000, train loss: 2.512, val loss: 2.552\nstep: 35000, train loss: 2.499, val loss: 2.532\nstep: 36000, train loss: 2.485, val loss: 2.537\nstep: 37000, train loss: 2.473, val loss: 2.522\nstep: 38000, train loss: 2.480, val loss: 2.523\nstep: 39000, train loss: 2.485, val loss: 2.524\nstep: 40000, train loss: 2.467, val loss: 2.510\nstep: 41000, train loss: 2.462, val loss: 2.512\nstep: 42000, train loss: 2.465, val loss: 2.512\nstep: 43000, train loss: 2.457, val loss: 2.507\nstep: 44000, train loss: 2.465, val loss: 2.497\nstep: 45000, train loss: 2.454, val loss: 2.497\nstep: 46000, train loss: 2.458, val loss: 2.505\nstep: 47000, train loss: 2.461, val loss: 2.495\nstep: 48000, train loss: 2.458, val loss: 2.492\nstep: 49000, train loss: 2.463, val loss: 2.494\nstep: 50000, train loss: 2.443, val loss: 2.491\nstep: 51000, train loss: 2.452, val loss: 2.490\nstep: 52000, train loss: 2.455, val loss: 2.501\nstep: 53000, train loss: 2.452, val loss: 2.502\nstep: 54000, train loss: 2.441, val loss: 2.498\nstep: 55000, train loss: 2.435, val loss: 2.490\nstep: 56000, train loss: 2.434, val loss: 2.494\nstep: 57000, train loss: 2.432, val loss: 2.488\nstep: 58000, train loss: 2.435, val loss: 2.489\nstep: 59000, train loss: 2.431, val loss: 2.488\nstep: 60000, train loss: 2.445, val loss: 2.488\nstep: 61000, train loss: 2.437, val loss: 2.485\nstep: 62000, train loss: 2.439, val loss: 2.480\nstep: 63000, train loss: 2.440, val loss: 2.484\nstep: 64000, train loss: 2.435, val loss: 2.481\nstep: 65000, train loss: 2.438, val loss: 2.482\nstep: 66000, train loss: 2.431, val loss: 2.489\nstep: 67000, train loss: 2.434, val loss: 2.479\nstep: 68000, train loss: 2.428, val loss: 2.491\nstep: 69000, train loss: 2.433, val loss: 2.483\nstep: 70000, train loss: 2.441, val loss: 2.488\nstep: 71000, train loss: 2.425, val loss: 2.475\nstep: 72000, train loss: 2.425, val loss: 2.490\nstep: 73000, train loss: 2.430, val loss: 2.470\nstep: 74000, train loss: 2.428, val loss: 2.472\nstep: 75000, train loss: 2.433, val loss: 2.481\nstep: 76000, train loss: 2.432, val loss: 2.477\nstep: 77000, train loss: 2.437, val loss: 2.474\nstep: 78000, train loss: 2.438, val loss: 2.483\nstep: 79000, train loss: 2.431, val loss: 2.472\nstep: 80000, train loss: 2.428, val loss: 2.484\nstep: 81000, train loss: 2.421, val loss: 2.488\nstep: 82000, train loss: 2.433, val loss: 2.477\nstep: 83000, train loss: 2.432, val loss: 2.486\nstep: 84000, train loss: 2.417, val loss: 2.480\nstep: 85000, train loss: 2.421, val loss: 2.478\nstep: 86000, train loss: 2.428, val loss: 2.482\nstep: 87000, train loss: 2.430, val loss: 2.493\nstep: 88000, train loss: 2.426, val loss: 2.470\nstep: 89000, train loss: 2.433, val loss: 2.476\nstep: 90000, train loss: 2.432, val loss: 2.479\nstep: 91000, train loss: 2.420, val loss: 2.473\nstep: 92000, train loss: 2.422, val loss: 2.481\nstep: 93000, train loss: 2.440, val loss: 2.481\nstep: 94000, train loss: 2.427, val loss: 2.483\nstep: 95000, train loss: 2.430, val loss: 2.485\nstep: 96000, train loss: 2.437, val loss: 2.477\nstep: 97000, train loss: 2.432, val loss: 2.476\nstep: 98000, train loss: 2.418, val loss: 2.477\nstep: 99000, train loss: 2.424, val loss: 2.471\n2.9871153831481934\n","output_type":"stream"}]},{"cell_type":"code","source":"context = torch.zeros((1,1), dtype=torch.long, device=device)\ngenerated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\nprint(generated_chars)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:14:06.332145Z","iopub.execute_input":"2023-11-15T07:14:06.332866Z","iopub.status.idle":"2023-11-15T07:14:06.469501Z","shell.execute_reply.started":"2023-11-15T07:14:06.332832Z","shell.execute_reply":"2023-11-15T07:14:06.468666Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"\n\"Wild an hekincres thicthas unca oin the wid curontizand waled-TEman'tissn aw, te s be o b, s, til thinsacomug g hith igimy-Andvecetit ate bupand the!*Q_by.\n\" thtenene eoustht sed ewomure d\n\nZe hee ve crtase, Jisth,\"\n\n\ns whee went a deshe Gay, t ha ulotoreo velyoas t gres oplo. se ff\n\" ors apordrs boimass.\n\" cuth uikeanl thesomiveron't sshexpreang f s um t veghance \"Bu m-oun, ty Dourrannorowembath t band t. h ge bry-b.\n\n\"So as, thouran\n\nThor ir\n\nwire hesu hene. wof whace bas'muttter\n\"\nsestingsth\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}